# Ollama Model Catalog (Crawler + Normalizer + VRAM Estimator)

This repo bootstraps the **catalog ingestion** layer for your "Ollama model helper, but useful" project.

What you get:
- A polite crawler that scrapes the Ollama Library:
  - `https://ollama.com/library`
  - `https://ollama.com/library/<slug>/tags`
- Normalization into Postgres tables
- A first-pass **VRAM estimator** (catalog-only, clearly labeled *Estimated*)
- A minimal derived-estimates writer (optional)

## Quick start (Docker)

1) Copy env:
```bash
cp .env.example .env
```

2) Start Postgres:
```bash
docker compose up -d db
```

3) Run migration:
```bash
docker compose run --rm crawler psql "$DATABASE_URL" -f /app/migrations/001_init.sql
```

4) Crawl + upsert:
```bash
docker compose run --rm crawler python -m crawler.main --estimate
```

## Run locally (no Docker)

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/ollama_catalog"
python -m crawler.main --estimate
```

## Notes

- **Release date model**
  - `upstream_published_at` (nullable) is set by a separate enrichment job later (HF / paper / admin).
  - `catalog_first_seen_at` is always non-null and initialized to `upstream_published_at` when known; otherwise it starts as the ingestion timestamp.
  - A DB trigger will pull `catalog_first_seen_at` back to `upstream_published_at` if an earlier upstream date is later added.

- **Verification everywhere**
  - `model_family` and `model_variant` are `catalog` verified (source-derived).
  - `derived_estimate` starts as `estimated`.
  - You can later promote to `community_verified` or `admin_verified`.


## GitHub Pages (host a functional site)

This repo can deploy a **static** site to GitHub Pages that:
- loads `site/data/catalog.json` (generated by Actions)
- lets users filter by **VRAM budget + context**, and rank by **workflow + toolchain community signal** (when available)

### One-time setup
1. Push this repo to GitHub.
2. In GitHub: **Settings → Pages → Build and deployment → Source → GitHub Actions**. citeturn0search5
3. Run the workflow manually once (Actions tab → `Deploy Pages` → Run workflow), or just push to `main`.

### How the Pages deployment works
- The workflow:
  1) spins up Postgres as a service
  2) applies `migrations/001_init.sql`
  3) crawls Ollama Library + tags pages
  4) computes first-pass VRAM estimate components (estimated)
  5) exports JSON to `site/data/catalog.json`
  6) deploys `site/` to GitHub Pages using the official Pages actions citeturn0search1turn0search0turn0search7

> **Important:** GitHub Pages is static. There is no live DB/API in this setup.
> For a live, writeable “community submissions” site, you’ll deploy the DB + API elsewhere, and keep Pages as a frontend.
